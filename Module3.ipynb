{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f4ecd9-75e2-4bf0-b36e-9a0bfe9ab174",
   "metadata": {},
   "source": [
    "# Module 3 - Building Neural Network with Input Data and Performing New Prediction\n",
    "\n",
    "In order to execute this module, files generated from module 1 are needed:\n",
    "\n",
    "CombinedAnalysis.xlsx --> USED FOR BUILDING NEURAL NET\n",
    "meanAllsigma.xlsx --> USED FOR CONSTRUCTING IONOGRAMS OF PREDICTED LIPIDS\n",
    "Intersected gauss.xlsx\n",
    "\n",
    "This module is used to perform predictions on novel lipid isomer pairs.\n",
    "The prediction output will be an excel file titled \"Predicted Intersected Gauss.xlsx\" which lists all the lipids user wishes to perform predictions on, each SV from 0 to 4100V at increment of 100V, and the predicted optimal CoV for each isomer. Output will also contain a column predicting the CoV at which the ionograms of the two isomers will intersect, and whether this intersection point is considered as \"Separable\" or \"Inseparable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59f26eb-1302-47ac-9159-cbf2c3dc3692",
   "metadata": {},
   "source": [
    "## INSTALLATION OF IMPORTANT LIBRARIES AND PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4759c12-5c4f-4021-92c0-5e35998c345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Import basic system parameters and functions \n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "# Several packages needed to be installed. We first will check to see if your environment has already had these packages installed.\n",
    "required = {'pandas', 'seaborn', 'numpy', 'csaps', 'tensorflow', 'openpyxl', 'lmfit', 'scikit-learn'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "\n",
    "if missing:\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout = subprocess.DEVNULL)\n",
    "\n",
    "SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True    \n",
    "\n",
    "# After checking, we now will import from the installed packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pylab as plb\n",
    "import matplotlib.pyplot as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import mlab\n",
    "import numpy as np\n",
    "import heapq\n",
    "import csaps\n",
    "import shutil\n",
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    "from csaps import csaps\n",
    "from random import seed\n",
    "from random import choice\n",
    "from numpy import asarray as ar\n",
    "from numpy import exp, linspace, random, pi, sqrt\n",
    "from scipy import stats\n",
    "from scipy import interpolate\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import pkg_resources\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import subprocess\n",
    "import tensorflow as tn\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from lmfit import Model, conf_interval, report_ci\n",
    "from lmfit.models import ExpressionModel\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Set Directory Location\n",
    "currentpath = os.getcwd()\n",
    "os.chdir(currentpath)\n",
    "\n",
    "# Set up folder for outputs and plots\n",
    "if not os.path.exists('Module3_Outputs'):\n",
    "    os.makedirs(os.path.join('Module3_Outputs', 'PredictedSetPlots'))\n",
    "    os.chmod('Module3_Outputs/PredictedSetPlots',0o666)\n",
    "else:\n",
    "    os.chmod('Module3_Outputs/PredictedSetPlots',0o666)\n",
    "    shutil.rmtree('Module3_Outputs/PredictedSetPlots')\n",
    "    os.makedirs(os.path.join(\"Module3_Outputs\", \"PredictedSetPlots\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36b7ece-a304-4120-9a3b-f7c7377bd418",
   "metadata": {},
   "source": [
    "## STEP 1. Import the required files\n",
    "\n",
    "Files generated from Module 1 are imported. All lipids in files generated in Module 1 are used for Training Dataset. Users are then prompted to submit an excel file listing lipids which they wish to predict for. The excel file must contain only 1 column, with column heading (hence, cell A1 of excel sheet will be ignored). For convenience, user is advised to use the provided template to generate their list of lipids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fcc66e-bad3-4814-bd83-69f8307b71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanAllsigma = pd.read_excel('Module1_Outputs/meanAllsigma.xlsx')\n",
    "CombinedAnalysis = pd.read_excel('Module1_Outputs/CombinedAnalysis.xlsx')\n",
    "Intersected_gauss = pd.read_excel('Module1_Outputs/Intersected gauss.xlsx')\n",
    "\n",
    "# Grab the Lipid List and turn to a list\n",
    "Trainlist_Lipids = list(pd.unique(CombinedAnalysis['Lipid Species']))\n",
    "\n",
    "# Output the number of unique lipid from this dataset\n",
    "n_TrainLipids = len(Trainlist_Lipids)\n",
    "\n",
    "# Talk to users\n",
    "print(\"There are \", n_TrainLipids, \"unique pairs of lipid isomers in your dataset.\" )\n",
    "print(\"All of these pairs of lipid isomers will be used for training dataset.\")\n",
    "\n",
    "# Select excel file listing lipids to be predicted for.\n",
    "## Files are read and stored as DataFrame.\n",
    "root = tkinter.Tk()\n",
    "InputFile3 = filedialog.askopenfile(parent=root,\n",
    "                                   mode = 'rb',\n",
    "                                   title = \"Please choose .xlsx file listing Lipids to be predicted for.\")\n",
    "if InputFile3 != None:\n",
    "    Predlist_Lipids = pd.read_excel(InputFile3) # Save as dataframe\n",
    "\n",
    "root.withdraw()\n",
    "root.update()\n",
    "\n",
    "# Turn the only column in dataframe to list\n",
    "\n",
    "Predlist_Lipids = Predlist_Lipids.iloc[:,0].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bab524-0763-477f-9cc0-c2815279954c",
   "metadata": {},
   "source": [
    "## STEP 2: Set up the files and seeding\n",
    "\n",
    "Create Input DataSet from Trainlist_Lipids and Predlist_Lipids. This step will create excel sheets of TrainingSet and PredictionSet, of lipids to be predicted for defined by user above, and information associated to them obtained from files generated from Module 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629af64-7ae9-4ada-97ed-baeeeccee165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TrainingSet dataframe and TrainingSet.xlsx\n",
    "TrainingSet = []\n",
    "for i in range(len(Trainlist_Lipids)):\n",
    "    temp_TrainingSet = CombinedAnalysis.loc[CombinedAnalysis['Lipid Species']==Trainlist_Lipids[i]]\n",
    "    TrainingSet.append(temp_TrainingSet)\n",
    "TrainingSet = pd.concat(TrainingSet)\n",
    "TrainingSet.reset_index(drop = True, inplace = True)\n",
    "TrainingSet = TrainingSet.drop(TrainingSet.columns[0], axis=1)\n",
    "\n",
    "\n",
    "if_na = TrainingSet[\"mu_Glc\"].isnull().values.any() or TrainingSet[\"mu_Gal\"].isnull().values.any()\n",
    "total_na = TrainingSet.isnull().sum().sum()\n",
    "\n",
    "if if_na == True:\n",
    "    print(\"You have NA for CoV values in your dataset. There are \", total_na, \" instances of NA values.\")\n",
    "    print(\"At the SV where there are NAs for CoV values of either Glc or Gal isomer, the entire SV will be removed for that pair of lipid isomers.\")\n",
    "    print(\"TrainingSet is now generated without any NAs.\")\n",
    "    input(\"Please press 'Enter' to continue.\")\n",
    "    TrainingSet = TrainingSet.dropna()\n",
    "    TrainingSet.reset_index(drop = True, inplace = True)\n",
    "\n",
    "TrainingSet.to_excel('Module3_Outputs/TrainingSet.xlsx')\n",
    "\n",
    "# Create PredictionSet dataframe and PredictionSet.xlsx. The lipids here were specified by users as Predlist_Lipids. The columns are\n",
    "# empty and will be occupied with predicted value after running through the neural net.\n",
    "\n",
    "PredictionSet = pd.DataFrame()                                                      \n",
    "for lipid in range(len(Predlist_Lipids)):\n",
    "    temp_PredictionSet = pd.DataFrame({'Lipid Species' : Predlist_Lipids[lipid],\n",
    "                                       'SV' : meanAllsigma['SV'],\n",
    "                                       'chain length' : ((Predlist_Lipids[lipid]).split(':'))[0],\n",
    "                                       'degree of unsaturation' : ((Predlist_Lipids[lipid]).split(':'))[1]\n",
    "                                       })  \n",
    "    PredictionSet = pd.concat([PredictionSet, temp_PredictionSet])\n",
    "\n",
    "PredictionSet['mu_Gal']  = ''\n",
    "PredictionSet['mu_Glc']  = ''\n",
    "PredictionSet['sigma_Gal']  = ''\n",
    "PredictionSet['sigma_Glc']  = ''\n",
    "PredictionSet.reset_index(drop = True, inplace = True)\n",
    "PredictionSet.to_excel('Module3_Outputs/PredictionSet.xlsx')\n",
    "\n",
    "# Seeding random number generator in tensorflow. Users can change this if wished to.\n",
    "tn.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990a8aba-e0cc-4d3e-a880-373cf8899072",
   "metadata": {},
   "source": [
    "## STEP 3. Standardize training dataset and set up hyperparameters\n",
    "\n",
    "Training Dataset will be standardized for lipid features by performing z-scoring. Lipid features are lipids' chain length and degree of unsaturation.\n",
    "Hyperparameters for the neural network are defined here based on previous optimization work. Hyperparameters may be changed by users if wished to, to test out other parameters applicable to neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa2c60-da9a-40b8-a9bd-5f276bc88307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. Set up the Trainining files and standardize training data input\n",
    "(nInstances, nCols) = TrainingSet.shape\n",
    "LipidFeatures = {'chain length', 'degree of unsaturation'}\n",
    "nLipidFeatures= len(LipidFeatures)\n",
    "LipidIsomers = {'Gal', 'Glc'}\n",
    "nLipidIsomers = len(LipidIsomers)\n",
    "\n",
    "## Set up file for X and Y variables of training dataset\n",
    "TrainX = np.zeros(shape=(nInstances, nLipidFeatures+1)) #number of rows is taken from nInstances; number of cols is lipid features + SV\n",
    "TrainY = np.zeros(shape=(nInstances, 2))\n",
    "\n",
    "for i in range(nInstances):\n",
    "    Temp = TrainingSet.loc[i,'Lipid Species']\n",
    "    Temp2 = Temp.split(':')\n",
    "    \n",
    "    for j in range(nLipidFeatures):\n",
    "        TrainX[i, j] = float(Temp2[j])\n",
    "        \n",
    "    TrainX[i, nLipidFeatures] = TrainingSet.loc[i,'SV']\n",
    "    TrainY[i, 0] = TrainingSet.loc[i, 'mu_Gal']\n",
    "    TrainY[i, 1] = TrainingSet.loc[i, 'mu_Glc']\n",
    "\n",
    "## Standardize lipid features by z-scoring\n",
    "FeatureMeans = np.mean(TrainX, axis=0)\n",
    "FeatureSD = np.std(TrainX, axis=0)\n",
    "for i in range(nLipidFeatures+1):\n",
    "    TrainX[:,i] = TrainX[:,i]-FeatureMeans[i]\n",
    "    TrainX[:,i] = TrainX[:,i]/FeatureSD[i]\n",
    "\n",
    "# ii. Set up Prediction file and standardize input data required for prediction\n",
    "(nPredInstances, nCols) = PredictionSet.shape\n",
    "PredX = np.zeros(shape=(nPredInstances, nLipidFeatures+1)) #number of rows is taken from nPredInstances; number of cols is lipid features + SV\n",
    "\n",
    "for i in range(nPredInstances):\n",
    "    Temp = PredictionSet.loc[i,'Lipid Species']\n",
    "    Temp2 = Temp.split(':')\n",
    "    \n",
    "    for j in range(nLipidFeatures):\n",
    "        PredX[i, j] = float(Temp2[j])\n",
    "        PredX[i, nLipidFeatures] = PredictionSet.loc[i,'SV']      \n",
    "    \n",
    "## Standardize lipid features by z-scoring\n",
    "for i in range(nLipidFeatures+1):\n",
    "    PredX[:,i] = PredX[:,i]-FeatureMeans[i]\n",
    "    PredX[:,i] = PredX[:,i]/FeatureSD[i]\n",
    "\n",
    "# iii. Hyperparameter Setup. Users can change if wished but iDMS work has shown that the followings are the most optimal parameters. \n",
    "Initializer = 'glorot_uniform'           #'he_uniform' \n",
    "HiddenLayers = 5\n",
    "InnerActivation = 'softplus'\n",
    "OuterActivation = 'linear'\n",
    "LossFunction = 'MAE'\n",
    "StopLoss = 1E-10\n",
    "\n",
    "print(\"The optimal network used here has \" +str(HiddenLayers)+ \" layers, with \" +InnerActivation+ \" as activation function for all layers, \" \n",
    "      +LossFunction+ \" or 'mean absolute error' as the loss function, initialized weights basd on the \" +Initializer+ \n",
    "      \" method, and an early stopping loss of 1E-10 \" )\n",
    "\n",
    "parameters = Initializer + \"_\" + str(HiddenLayers) + \"_\" + InnerActivation + \"_\" + OuterActivation + \"_\" + LossFunction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bd1bb-f73d-4fbc-aa5c-05d9cb6a8a5b",
   "metadata": {},
   "source": [
    "## STEP 4. Model Initialization\n",
    "\n",
    "Creating a neural network model based on the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848364f-5c2a-474b-a090-832833873068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the number of units in each hidden layer. This has been optimized and determined to be 10.\n",
    "nUnitsPerHiddenLayer = 10\n",
    "\n",
    "# Create the model\n",
    "ourModel = models.Sequential()\n",
    "\n",
    "# Add the hidden layers. The first layer input are lipid features (acyl chain, # of unsaturation) and SV.\n",
    "ourModel.add(layers.Dense(units = nUnitsPerHiddenLayer, kernel_initializer = Initializer, input_dim = nLipidFeatures+1, \n",
    "                          activation = InnerActivation))\n",
    "\n",
    "# Remaining hidden layers are similar, but have NUnitsPerHiddenLayer input dimension\n",
    "for i in range(HiddenLayers-1):\n",
    "    ourModel.add(layers.Dense(units = nUnitsPerHiddenLayer, kernel_initializer = Initializer, input_dim = nUnitsPerHiddenLayer, \n",
    "                              activation = InnerActivation))\n",
    "\n",
    "# Create Output layer with two output units (Glc Peak CoV(or Glc_mu), and Gal Peak CoV(or Gal_mu))\n",
    "ourModel.add(layers.Dense(units = 2, kernel_initializer = Initializer, input_dim = nUnitsPerHiddenLayer, \n",
    "                          activation = OuterActivation))\n",
    "\n",
    "# Add loss function and optimizer to our model\n",
    "ourModel.compile(optimizer = 'adam', loss = LossFunction)\n",
    "\n",
    "# Train our model on the training dataset\n",
    "history = ourModel.fit(TrainX, TrainY, epochs=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0593d-dcf0-418b-8e90-1123edc222ac",
   "metadata": {},
   "source": [
    "## STEP 5. Prediction\n",
    "\n",
    "This code block calculates predictions for the PredictionInputs and outputs them to file PredictionSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22061302-2249-4e3e-95d8-5f190d6dcd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "PredY = ourModel.predict(PredX)\n",
    "\n",
    "# Insert into dataframe\n",
    "PredictionSet.loc[:,'mu_Gal'] = PredY[:,0]\n",
    "PredictionSet.loc[:,'mu_Glc'] = PredY[:,1]\n",
    "\n",
    "# Compile model asking for accuracy\n",
    "ourModel.compile(loss = 'MAE', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# Evaluate the model. Score output is [the loss (MAE), accuracy]\n",
    "score = ourModel.evaluate(TrainX, TrainY, verbose =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5785b-8a67-4fec-abe0-6d1ea11ed4a5",
   "metadata": {},
   "source": [
    " ## STEP 6. From predicted value, apply Gaussian functions, predict separation and construct confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39770729-5486-4039-a3fe-824c00105b90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add sigma values from mean sigma sheet\n",
    "minSV = meanAllsigma['SV'].min()\n",
    "maxSV = meanAllsigma['SV'].max()\n",
    "stepSV = int((maxSV - minSV) / ((meanAllsigma['SV'].nunique())-1))\n",
    "\n",
    "# Create a dictionary of the mean sigmas\n",
    "Gal_sigma_dict = pd.Series(meanAllsigma.sigma_Gal.values, index = meanAllsigma.SV).to_dict()\n",
    "Glc_sigma_dict = pd.Series(meanAllsigma.sigma_Glc.values, index = meanAllsigma.SV).to_dict()\n",
    "\n",
    "# Combine\n",
    "PredictionSet[\"sigma_Gal\"] = PredictionSet[\"SV\"].map(Gal_sigma_dict)\n",
    "PredictionSet[\"sigma_Glc\"] = PredictionSet[\"SV\"].map(Glc_sigma_dict)\n",
    "\n",
    "# Save it\n",
    "PredictionSet.to_excel(\"Module3_Outputs/PredictionSet_\"+parameters+\".xlsx\",index=False)\n",
    "\n",
    "# Apply Gaussian functions\n",
    "## Copy dataframe from PredictionSet and add a new column for climax intensity\n",
    "Gauss_PredictionSet = PredictionSet.copy()\n",
    "\n",
    "# Predict Intersection Point by Sigma and Gaussian\n",
    "pred_Intersected_gauss = pd.DataFrame()\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "   y = 1 * exp(-0.5*((x-mu)/sigma)**2)  #Y=Amplitude*exp(-0.5*((X-Mean)/SD)^2)\n",
    "   return y\n",
    "\n",
    "def solve(mu_Gal,mu_Glc,sig_Gal,sig_Glc): \n",
    "  a = 1/(2*sig_Gal**2) - 1/(2*sig_Glc**2)\n",
    "  b = mu_Glc/(sig_Glc**2) - mu_Gal/(sig_Gal**2)\n",
    "  c = mu_Gal**2 /(2*sig_Gal**2) - mu_Glc**2 / (2*sig_Glc**2) - np.log(sig_Glc/sig_Gal)\n",
    "  return np.roots([a,b,c])\n",
    "\n",
    "for i in range(len(Predlist_Lipids)):\n",
    "    pred_intersect_temp = (Gauss_PredictionSet[Gauss_PredictionSet['Lipid Species']==Predlist_Lipids[i]])\n",
    "    \n",
    "    minSV = pred_intersect_temp['SV'].min()\n",
    "    maxSV = pred_intersect_temp['SV'].max()\n",
    "    stepSV = int((maxSV - minSV) / ((pred_intersect_temp['SV'].nunique())-1))\n",
    "    \n",
    "       \n",
    "    for SV in range(minSV, maxSV+stepSV, stepSV):\n",
    "        pred_intersectSV_temp = pred_intersect_temp[pred_intersect_temp['SV']==SV]\n",
    "        \n",
    "        if pred_intersectSV_temp[\"mu_Gal\"].notna() is False:\n",
    "            continue\n",
    "        else:\n",
    "            mu_Gal = float(pred_intersectSV_temp[\"mu_Gal\"])\n",
    "            sig_Gal = float(pred_intersectSV_temp[\"sigma_Gal\"])\n",
    "        \n",
    "        if pred_intersectSV_temp[\"mu_Glc\"].notna() is False:\n",
    "            continue\n",
    "        else:\n",
    "            mu_Glc = float(pred_intersectSV_temp[\"mu_Glc\"])\n",
    "            sig_Glc = float(pred_intersectSV_temp[\"sigma_Glc\"])\n",
    "        \n",
    "        pred_gauss_features = [mu_Gal, mu_Glc, sig_Gal, sig_Glc] \n",
    "     \n",
    "        if any(pd.isna(pred_gauss_features)) is True:\n",
    "            continue\n",
    "        else:\n",
    "            pred_resultCombined = solve(mu_Gal, mu_Glc, sig_Gal, sig_Glc)\n",
    "        \n",
    "        \n",
    "        x = np.linspace(-10,10,10000) #np.linspace(mu_Gal - 3*sig_Gal, mu_Gal + 3*sig_Gal, 100)\n",
    "        y_Gal_pred = gaussian(x, mu_Gal, sig_Gal)\n",
    "        y_Glc_pred = gaussian(x, mu_Glc, sig_Glc)\n",
    "        \n",
    "        # Intersection point is limited to only that lies between mu of Glc and Gal\n",
    "        mu_range_max = max(mu_Glc, mu_Gal)\n",
    "        mu_range_min = min(mu_Glc, mu_Gal)\n",
    "        \n",
    "        pred_intersect = max([point for point in pred_resultCombined if mu_range_min < point < mu_range_max], default = \"NAN\")\n",
    "        \n",
    "        if type(pred_intersect) is str:\n",
    "            y_at_intersect = pred_intersect            \n",
    "        else:\n",
    "            y_at_intersect = gaussian(pred_intersect, mu_Gal, sig_Gal)\n",
    "          \n",
    "        if pred_intersect != \"NAN\" and (y_at_intersect < 0.5 ):\n",
    "           pred_intersectSV_temp[\"Valley CoV\"] = pred_intersect\n",
    "       \n",
    "        pred_intersectSV_temp[\"Intersection point\"] = pred_intersect\n",
    "        pred_intersectSV_temp[\"Norm Intensity at Intersection\"] = y_at_intersect\n",
    "         \n",
    "         \n",
    "        ax = []\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlim([-10, 10])\n",
    "\n",
    "        #plt.figure()\n",
    "        titlelipid = 'Gaussian distribution plot based on predicted mu and sigma of \\n GlcCer(d18:1/' + str(Predlist_Lipids[i]) + ' and GalCer(d18:1/' + str(Predlist_Lipids[i]) + ') at SV ' + str(SV)\n",
    "        plt.ylabel('Predicted Normalized Signal Intensity')\n",
    "        plt.xlabel('CoV')\n",
    "        plt.title(titlelipid)\n",
    "        plot1 = plt.plot(x, y_Gal_pred, color='green', label='GalCer(d18:1/' + str(Predlist_Lipids[i]) + ')' )\n",
    "        plot2 = plt.plot(x, y_Glc_pred, color='blue', label='GlcCer(d18:1/' + str(Predlist_Lipids[i]) + ')' )\n",
    "         \n",
    "        if pred_intersect != \"NAN\":\n",
    "          plot3 = plt.plot(pred_intersect, y_at_intersect, 'ro', fillstyle='none', label = 'Intersection point')\n",
    "          \n",
    "        plt.legend(loc='best')\n",
    "        temp_plot = plt.gcf()\n",
    "        plt.show()\n",
    "        plt.draw()\n",
    "        \n",
    "        plotname = 'Gaussian_Glc' + str(Predlist_Lipids[i]) + '_Gal' + str(Predlist_Lipids[i]) + '_SV' + str(SV)\n",
    "        plotname = plotname.replace(\":\",\"-\")\n",
    "        plotpath = os.path.join(currentpath, 'Module3_Outputs/PredictedSetPlots',plotname)\n",
    "        temp_plot.savefig(plotpath)\n",
    "            \n",
    "        pred_Intersected_gauss = pd.concat([pred_Intersected_gauss, pred_intersectSV_temp])\n",
    "        pred_Intersected_gauss.reset_index(drop = True, inplace = True)\n",
    "\n",
    "plt.close()\n",
    "pred_Intersected_gauss[\"Lipid Species\"] = pred_Intersected_gauss['chain length'].astype(str) + ':' + pred_Intersected_gauss['degree of unsaturation'].astype(str)\n",
    "\n",
    "# Binary Separation Column\n",
    "\n",
    "pred_Intersected_gauss['Sep or InSep'] = np.where(pred_Intersected_gauss['Valley CoV'].isnull(), \"Inseparable\", \"Separable\")\n",
    "\n",
    "        \n",
    "# Output intersection point to files\n",
    "pred_Intersected_gauss = pred_Intersected_gauss.replace('NAN','')\n",
    "pred_Intersected_gauss.to_excel('Module3_Outputs/PredictionResult.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48ea27-334e-4482-a354-50cafa627663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finished Module 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ea59c-722d-480c-a645-b72611df88bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd578e7-b14d-4a67-b05f-d85e7a52051e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07480c18-82ba-4018-aa59-56eeedeeff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
